{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6dedd34-ea28-4a7b-8208-d7cd19af7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {c:i+1 for i,c in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d4115d2d-cb7e-4073-9e22-666880ea01be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create training set\n",
    "# we want to predict bigram ch1,ch2  \n",
    "#x=ch1 (what we take as input to nn)\n",
    "#y=ch2 (what we want to predict)\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "# tensor need one hot / (float or int..) not strs..\n",
    "# tensor.Tensor float, tensor.tensor infers dtype\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"Number of examples: \", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b057a5-0fb0-41d2-b09d-2b9a24fbac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True) # drawn from normal dist. with mean=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "039ecc56-6dd9-43d7-906a-414dc5c446ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.76861834526062\n",
      "3.3788065910339355\n",
      "3.161090850830078\n",
      "3.0271859169006348\n",
      "2.9344842433929443\n",
      "2.867231607437134\n",
      "2.8166542053222656\n",
      "2.777146339416504\n",
      "2.7452542781829834\n",
      "2.7188305854797363\n",
      "2.696505546569824\n",
      "2.6773719787597656\n",
      "2.6608054637908936\n",
      "2.6463515758514404\n",
      "2.633665084838867\n",
      "2.622471570968628\n",
      "2.6125476360321045\n",
      "2.6037068367004395\n",
      "2.595794439315796\n",
      "2.5886809825897217\n",
      "2.582256317138672\n",
      "2.5764293670654297\n",
      "2.5711236000061035\n",
      "2.566272735595703\n",
      "2.5618226528167725\n",
      "2.5577261447906494\n",
      "2.5539445877075195\n",
      "2.550442695617676\n",
      "2.5471930503845215\n",
      "2.5441696643829346\n",
      "2.5413525104522705\n",
      "2.538722038269043\n",
      "2.536262035369873\n",
      "2.5339579582214355\n",
      "2.531797409057617\n",
      "2.5297679901123047\n",
      "2.527860164642334\n",
      "2.526063919067383\n",
      "2.5243709087371826\n",
      "2.522773265838623\n",
      "2.52126407623291\n",
      "2.519836664199829\n",
      "2.5184855461120605\n",
      "2.517204999923706\n",
      "2.515990972518921\n",
      "2.5148372650146484\n",
      "2.5137410163879395\n",
      "2.512698173522949\n",
      "2.511704444885254\n",
      "2.5107579231262207\n",
      "2.509855031967163\n",
      "2.5089924335479736\n",
      "2.5081682205200195\n",
      "2.5073797702789307\n",
      "2.5066258907318115\n",
      "2.5059030055999756\n",
      "2.5052106380462646\n",
      "2.5045459270477295\n",
      "2.5039076805114746\n",
      "2.503295421600342\n",
      "2.5027060508728027\n",
      "2.5021398067474365\n",
      "2.501594305038452\n",
      "2.5010693073272705\n",
      "2.500563383102417\n",
      "2.500075101852417\n",
      "2.4996049404144287\n",
      "2.4991507530212402\n",
      "2.4987120628356934\n",
      "2.49828839302063\n",
      "2.4978790283203125\n",
      "2.4974827766418457\n",
      "2.4970996379852295\n",
      "2.4967291355133057\n",
      "2.496370315551758\n",
      "2.496022939682007\n",
      "2.4956860542297363\n",
      "2.4953596591949463\n",
      "2.4950432777404785\n",
      "2.4947361946105957\n",
      "2.494438886642456\n",
      "2.494149684906006\n",
      "2.4938690662384033\n",
      "2.4935967922210693\n",
      "2.4933323860168457\n",
      "2.493075132369995\n",
      "2.4928252696990967\n",
      "2.492582321166992\n",
      "2.4923462867736816\n",
      "2.492116689682007\n",
      "2.4918930530548096\n",
      "2.491675853729248\n",
      "2.491464614868164\n",
      "2.491258382797241\n",
      "2.491058349609375\n",
      "2.4908626079559326\n",
      "2.4906723499298096\n",
      "2.4904870986938477\n",
      "2.4903063774108887\n",
      "2.4901304244995117\n"
     ]
    }
   ],
   "source": [
    "# grad desc\n",
    "alpha = 50\n",
    "for k in range(100):\n",
    "    # each row in xenc @ W gives us probabilities of what comes after x_i for each of 27 possible next char. \n",
    "    # lets interpret it as log counts, then we take exp (to give us bet 0-1 and then we can take softmax\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W # log-counts\n",
    "    counts = (xenc @ W).exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probs for next char given example\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # reg 1/N (W^2)\n",
    "    print(loss.item())\n",
    "    #backward pass\n",
    "    W.grad = None # set to zero\n",
    "    loss.backward() #pytorch keeps computational graph\n",
    "    #update\n",
    "    W.data += -alpha * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543893ad-fa19-41bf-a6ea-559a3a3828d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we expect it to converge to?\n",
    "# Lower NLL better.. but whats the best we can do? That is the probability of observing the data we are given, that is the maximum likelihood\n",
    "# 2.47 loss is the same loss we got when we empirical counted and calculated our NLL. that is the prob of our observed data.\n",
    "# The NN method scales much better.. dont need to load all the \n",
    "\n",
    "# The weights is exactly the logits of the counts that we counted!! same 27,27 arr\n",
    "# We did smoothing in the other e.g add 1 laplace to aovid 0 prob for the log. \n",
    "# The more add add, the more 'smoothed' the distribution because and the predictions are same\n",
    "# Similar idea happens when all Weights are 0. exp that -> 1 -> all probs equal chance\n",
    "# weights closer to 0 max entropy zero implies \"no opinion\" or \"maximum uncertainty.\", helps prevent overfitting\n",
    "#  Small weights force the model to be smoother and more general.\n",
    "# that is L1/L2 regulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ba515426-cde3-4e35-90b4-e39960b5f2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# samping from NN model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
