{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a6874eb-cd1c-49a6-b209-175b7e81907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13300c7-3468-4795-b792-9780e0a9a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0745fc16-f99f-478e-bd82-d6d292f84e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2891dd9f-0f7a-4444-8a5a-a830f815b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {s:i for i,s in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fa47971f-4133-401f-b484-729eb7b6ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "# emma\n",
    "# ... ---> e\n",
    "# ..e ---> m\n",
    "# .em ---> m\n",
    "# emm ---> a\n",
    "# mma ---> .\n",
    "# olivia\n",
    "# ... ---> o\n",
    "# ..o ---> l\n",
    "# .ol ---> i\n",
    "# oli ---> v\n",
    "# liv ---> i\n",
    "# ivi ---> a\n",
    "# via ---> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "afd99759-c1f6-49d9-b50f-2fed83476cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]), torch.int64, torch.int64)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "af150704-2b0d-4ed9-be8d-5c3ca05bbf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2326,  1.1880],\n",
       "        [-0.2304, -0.5606]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C is a matrix of embeddings of dim = 2\n",
    "# Each word (char in alphabet -> assoc. w/ dim2 vector)\n",
    "C = torch.randn((27,2))\n",
    "C[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2769cff3-f83b-4cd9-9c9a-015b230a4045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 5]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4379dee2-5989-4aef-a464-9b2ebcdaed69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2326,  1.1880],\n",
       "         [ 0.2326,  1.1880],\n",
       "         [ 0.2326,  1.1880]],\n",
       "\n",
       "        [[ 0.2326,  1.1880],\n",
       "         [ 0.2326,  1.1880],\n",
       "         [-0.4565, -0.3454]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "36d4910d-2416-47b8-8b3a-2adcaf223f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2326,  1.1880,  0.2326,  1.1880,  0.2326,  1.1880],\n",
       "        [ 0.2326,  1.1880,  0.2326,  1.1880, -0.4565, -0.3454]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(-1,6)[:2] # yay lines up with what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dc7d652a-0a1d-44a5-bde4-9831d8730612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.rand((6,100))\n",
    "b = torch.rand(100)\n",
    "h = emb.view(-1,6) @ W + b\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a57afb96-f816-4246-af3b-6c4338e6ea70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3658,  2.3417,  2.5525,  ...,  3.1226,  1.5774,  3.2040],\n",
       "        [ 1.4455,  1.3229,  1.4431,  ...,  2.8763,  0.8807,  1.7893],\n",
       "        [-0.0219,  0.4716,  1.3550,  ...,  0.8436, -0.1669,  0.2259],\n",
       "        ...,\n",
       "        [ 1.3718,  1.0726,  0.9706,  ...,  1.2859,  0.4700,  1.9794],\n",
       "        [ 0.9575,  0.7180,  0.1896,  ...,  1.2350,  0.2434, -0.2742],\n",
       "        [-1.2576,  0.2493,  0.7507,  ...,  0.1993,  0.4221, -0.6640]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "41a0449c-e02f-42db-9d33-8a0f9f0c32a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.rand((100,27))\n",
    "b2 = torch.rand(27)\n",
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f8009b39-0a09-426d-907a-f7647b96f661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3412)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is not good approach numerical unstability happens if the logit > ~80\n",
    "# You can take away the max from each one of them but still .. \n",
    "# count = logits.exp()\n",
    "# prob = count / count.sum(1, keepdims=True)\n",
    "\n",
    "# numerically stable but still ineff... we want CE loss\n",
    "#prob = F.softmax(logits, dim=-1) \n",
    "#loss = -prob[torch.arange(32), Y].log().mean()\n",
    "#loss\n",
    "# just do this - doesn't create intermediate tensors saves memory. backward pass is more efficeint \n",
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f226ea-e2fc-488b-97a0-72e2c516b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serious only # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "c11496e9-8513-44ff-bd42-c72674644fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a9365c7b-853e-4af8-ab34-5ef2f66b6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator=g, requires_grad = True) #rand unfirm dist. (0,1) vs. randn gaussian dist mean 0\n",
    "W1 = torch.randn((6,100), generator=g, requires_grad = True)\n",
    "b1 = torch.randn(100, generator=g, requires_grad = True)\n",
    "W2 = torch.randn((100,27), generator=g, requires_grad = True)\n",
    "b2 = torch.randn(27, generator=g, requires_grad = True)\n",
    "params = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "bb2298cf-48d7-44cb-ad4f-957c86f405eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "602bf75c-8a4e-4e31-9f81-a49b87f50492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1202139854431152\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    # minibatch (approx. gradient)\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # 32 random row index's\n",
    "    emb = C[X[ix]] # (32,3,2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # -1 pytorch infers size, no need to say 32 as it might change, h.shape (32,100)\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    \n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        lr = 0.001 # can be found w graph of loss against lr\n",
    "        p.data += -lr * p.grad # how determine learning rate?\n",
    "print(loss.item()) # this wont be 0 since we have many different starts, ... -> a or ... -> e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "fad648c9-1fce-4b57-967f-3d7e62729edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower loss better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2f102-31b6-4a7b-80cf-6dc6a766592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
